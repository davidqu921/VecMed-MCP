import os
import requests
import json
from search_milvusdb_3 import search_pubmed_by_query

# ====== æ¨¡å‹é…ç½®è¯»å– ======
LLM_API_BASE = os.getenv("LLM_BINDING_HOST", "http://localhost:18081/v1")
LLM_API_KEY = os.getenv("LLM_BINDING_API_KEY", "")
LLM_MODEL = os.getenv("LLM_MODEL", "Qwen3-32B")
USE_TRANSLATION = True  # å¦‚æœä¸º Trueï¼Œå°è¯•å°†è¾“å‡ºç¿»è¯‘ä¸ºä¸­æ–‡

# ====== æ„é€  Prompt ======
def build_prompt(docs, user_query):
    prompt = f"""You are a medical research assistant. Based on the following PubMed articles retrieved in response to the query: "{user_query}", summarize the key findings, patterns, and any notable observations in 500 words or less.

    Return your summary in Chinese with well-structured bullet points or a concise paragraph. ä½ çš„å›ç­”åº”ä½¿ç”¨ä¸­æ–‡/no_thinkã€‚

    ### Articles:
    """
    for i, doc in enumerate(docs, 1):
        prompt += f"""
                    --- Article {i} ---
                    Title: {doc['title']}
                    Authors: {doc['authors']}
                    DOI: {doc['doi']}
                    Abstract: {doc['abstract']}
                """
    return prompt.strip()

# ====== è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹ (OpenAI API å…¼å®¹æ–¹å¼) ======
# def summarize_with_llm(prompt):
#     headers = {
#         "Content-Type": "application/json",
#         "Authorization": f"Bearer {LLM_API_KEY}"
#     }

#     payload = {
#         "model": LLM_MODEL,
#         "messages": [
#             {"role": "system", "content": "You are a helpful assistant for summarizing medical research, and also give the source articals' title, author names, Journal and year."},
#             {"role": "user", "content": prompt}
#         ],
#         "temperature": 0.7,
#         "enable_thinking": False,
#         # "stream": True
#     }

#     try:
#         response = requests.post(f"{LLM_API_BASE}/chat/completions", headers=headers, data=json.dumps(payload))
#         response.raise_for_status()
#         output = response.json()
#         return output["choices"][0]["message"]["content"]
#     except requests.exceptions.RequestException as e:
#         print("âŒ è¯·æ±‚å¤±è´¥:", str(e))
#         return None
#     except KeyError:
#         print("âŒ å“åº”æ ¼å¼å¼‚å¸¸: ", response.text)
#         return None

def summarize_with_llm(prompt: str):
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LLM_API_KEY}"
    }

    payload = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system",
             "content": "You are a helpful assistant for summarizing medical research, "
                        "and also give the source articles' title, author names, "
                        "Journal and year."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "stream": True,            # æ‰“å¼€æµå¼
        "extra_body": {"enable_thinking": False}   # å…³é—­æ€è€ƒé“¾ï¼ˆå•†ä¸šç‰ˆé»˜è®¤ Falseï¼‰
    }

    resp = requests.post(
        f"{LLM_API_BASE}/chat/completions",
        headers=headers,
        data=json.dumps(payload),
        stream=True,             
        timeout=160
    )
    resp.raise_for_status()

    full_content = ""
    for raw_line in resp.iter_lines(decode_unicode=True):
        # SSE æ•°æ®æ ¼å¼ï¼šdata: {...}  æˆ–  data: [DONE]
        if not raw_line or raw_line == "data: [DONE]":
            continue
        if raw_line.startswith("data: "):
            try:
                data = json.loads(raw_line[6:])
                delta = data["choices"][0]["delta"]
                if delta.get("content"):
                    piece = delta["content"]
                    full_content += piece
                    print(piece, end="", flush=True)
            except Exception as e:
                print(f"[è§£æé”™è¯¯] {e}")
    return full_content

# ====== ç¿»è¯‘å‡½æ•°ï¼ˆä½¿ç”¨ prompt ç¿»è¯‘ï¼‰ ======
# def translate_to_chinese(text):
#     prompt = f"è¯·å°†ä»¥ä¸‹è‹±æ–‡åŒ»å­¦å†…å®¹ç¿»è¯‘æˆç®€æ´å‡†ç¡®çš„ä¸­æ–‡ï¼š\n\n{text}"
#     return summarize_with_llm(prompt)

# ====== ä¸»æ‰§è¡Œå…¥å£ ======
def main():
    user_query = input("è¯·è¾“å…¥ä½ çš„ç ”ç©¶é—®é¢˜æˆ–å…³é”®è¯ï¼š\n> ").strip()
    print("\nğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡çŒ®...")

    # è¿™é‡Œæ¨¡æ‹Ÿæ£€ç´¢ç»“æœï¼ˆä½ åº”æ›¿æ¢ä¸ºå®é™… Milvus æ£€ç´¢è°ƒç”¨ï¼‰
    top_articles = search_pubmed_by_query(user_query)
    if not top_articles:
        print("æœªæ‰¾åˆ°ç›¸å…³æ–‡ç« ã€‚")
        return
        
    print(f"\nâœ… æ‰¾åˆ° {len(top_articles )} ç¯‡ç›¸å…³æ–‡ç« ï¼Œæ­£åœ¨æ•´ç†ä¸­...\n")
    prompt = build_prompt(top_articles, user_query)
    
    summary = summarize_with_llm(prompt)
    if summary is None:
        print("âŒ æ— æ³•ç”Ÿæˆæ€»ç»“ã€‚")
        return

    # print("\nğŸ“„ æ€è€ƒä¸æ€»ç»“å¦‚ä¸‹ï¼š\n")
    # print(summary)

    # if USE_TRANSLATION:
    #     print("\nğŸŒ æ­£åœ¨ç¿»è¯‘ä¸ºä¸­æ–‡...\n")
    #     translated = translate_to_chinese(summary)
    #     print("\nğŸ“„ ä¸­æ–‡ç¿»è¯‘å¦‚ä¸‹ï¼š\n")
    #     print(translated if translated else "âš ï¸ ç¿»è¯‘å¤±è´¥ã€‚")

if __name__ == "__main__":
    main()
